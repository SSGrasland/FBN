pip install pdfplumber


pip install pdfkit


import pdfkit

def convert_html_to_pdf(html_file, output_pdf):
    try:
        pdfkit.from_file(html_file, output_pdf)
        print(f"PDF created successfully: {output_pdf}")
    except Exception as e:
        print(f"Error: {e}")

if __name__ == "__main__":
    html_file = r"C:\Users\SalomeGrasland\Desktop\FBN PDFs-20240319T133812Z-001\FBN PDFs\Volume 6\FBN Vol 6 No 1.pdf"
    output_pdf = r"C:\Users\SalomeGrasland\Desktop\FBN PDFs-20240319T133812Z-001\FBN PDFs\Volume 6\FBN Vol 6 No 1.pdf"
    convert_html_to_pdf(html_file, output_pdf)


import pdfplumber

with pdfplumber.open(r'C:\Users\SalomeGrasland\Desktop\FBN PDFs-20240319T133812Z-001\FBN PDFs\Volume 6\FBN Vol 6 No 2 Supplement.pdf') as pdf:
    for page in pdf.pages:
        text = page.extract_text()
        print(text)



text


with open('FBN Vol 6 No 1_text.txt', 'w', encoding='utf-8') as file:
    file.write(extracted_text)


import re

def extract_isbns(text):
    # Define a regex pattern to match strings of numbers with dashes
    isbn_pattern = r'\b\d{1,5}-?\d{1,7}-?\d{1,7}-?\d{1,7}-?\w\b'

    # Find all matches of the ISBN pattern in the text
    isbns = re.findall(isbn_pattern, text)

    return isbns

# Extract ISBNs from the example text
isbns_found = extract_isbns(extracted_text)

# Print the list of ISBNs
print("ISBNs found:", isbns_found)


import pdfplumber
import pandas as pd
import re

def extract_isbns_from_text(text):
    # Define the ISBN regex pattern
    isbn_pattern = re.compile(r'\b\d{1,5}-?\d{1,7}-?\d{1,7}-?\d{1,7}-?\w\b')

    # Find all ISBNs in the text
    isbns_found = isbn_pattern.findall(text)
    return isbns_found

def extract_text_from_pdf(pdf_path):
    data = {'PageNumber': [], 'Text': [], 'ISBNs': []}
    with pdfplumber.open(pdf_path) as pdf:
        for i, page in enumerate(pdf.pages, start=1):
            # Add page number and corresponding text to the data dictionary
            data['PageNumber'].append(i)
            text = page.extract_text()
            data['Text'].append(text)
            
            # Extract ISBNs from the text
            isbns_found = extract_isbns_from_text(text)
            data['ISBNs'].append(isbns_found)

    return pd.DataFrame(data)

# Replace 'your_pdf_file.pdf' with the actual path to your PDF file
pdf_path = r'C:\Users\SalomeGrasland\Desktop\FBN PDFs-20240319T133812Z-001\FBN PDFs\Volume 6\FBN Vol 6 No 1.pdf'
df = extract_text_from_pdf(pdf_path)



df.head()


df.to_csv('20.4_text.csv', index=False)


import requests
import csv

api_key = '52084_9ddf62aeaa4a39b5485d9b7fb69dd5a8'
isbns_to_process = isbns_found  # Replace with the original list of ISBNs you want to process

# Construct the base URL
base_url = 'https://api2.isbndb.com/book/'

# Create a CSV file and write headers
with open('book_information.csv', 'w', newline='', encoding='utf-8') as csv_file:
    fieldnames = ['ISBN', 'Title', 'Author', 'Publisher', 'Pages', 'Date Published', 'Binding', 'Synopsis', 'Language', 'Edition', 'Dimensions', 'MSRP', 'Image', 'Status']
    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)

    writer.writeheader()

    # Make requests for each ISBN in the list
    for isbn in isbns_to_process:
        url = f'{base_url}{isbn}'
        headers = {'Authorization': api_key}

        response = requests.get(url, headers=headers)

        if response.status_code == 200:
            book_info = response.json().get('book', {})
            
            # Write information to CSV
            writer.writerow({
                'ISBN': isbn,
                'Title': book_info.get('title', ''),
                'Author': ', '.join(book_info.get('authors', [])),
                'Publisher': book_info.get('publisher', ''),
                'Pages': book_info.get('pages', ''),
                'Date Published': book_info.get('date_published', ''),
                'Binding': book_info.get('binding', ''),
                'Synopsis': book_info.get('synopsis', ''),
                'Language': book_info.get('language', ''),
                'Edition': book_info.get('edition', ''),
                'Dimensions': book_info.get('dimensions', ''),
                'MSRP': book_info.get('msrp', ''),
                'Image': book_info.get('image', ''),
                'Status': 'Success'
            })
        else:
            print(f"Error for ISBN {isbn}: {response.status_code}")

            # Write a row with the ISBN, an indication of the error status, and empty fields for other information
            writer.writerow({
                'ISBN': isbn,
                'Status': 'Error',
                'Title': '',
                'Author': '',
                'Publisher': '',
                'Pages': '',
                'Date Published': '',
                'Binding': '',
                'Synopsis': '',
                'Language': '',
                'Edition': '',
                'Dimensions': '',
                'MSRP': '',
                'Image': ''
            })

print("CSV file created successfully.")


df.head()


import pandas as pd

# Assuming 'ISBNs' is the current column name and 'ISBN' is the desired column name
df.rename(columns={'ISBNs': 'ISBN'}, inplace=True)


page = df[['PageNumber', 'ISBN']]
page.head()


page['ISBN'] = page['ISBN'].astype(str).str.replace('[\[\]\'\"]', '')
page.head()


# Split values on comma and explode to create new rows
page['ISBN'] = page['ISBN'].str.split(', ')
page = page.explode('ISBN')

# Display the updated DataFrame
page.head(10)


import pandas as pd

# Specify the file path
file_path = r'C:\Users\salom\OneDrive\Área de Trabalho\FBN-\data\book_information.csv'

# Read the CSV file into a DataFrame
info = pd.read_csv(file_path)

info


merge = pd.merge(info, page, on='ISBN', how='left')
merge


# Assuming 'merge' is your DataFrame
# Drop rows with NaN in 'Title' and specific ISBN formats
merge = merge[~(merge['Title'].isna() & merge['ISBN'].str.contains(r'^\d{1}-\d{3}-\d{3}-\d{4}$|^\d{3}-\d{3}-\d{4}$|^\d{5}-\d{4}$|^\d{5}$|^\d{6}$'))]

# Display the updated DataFrame
merge


import pandas as pd
duplicate = pd.read_csv(r"C:\Users\salom\OneDrive\Área de Trabalho\FBN\data\20.4\20.4_booklist.csv")
duplicate


duplicate = duplicate.drop_duplicates(subset=['ISBN'])
duplicate


# Assuming 'merge' is your DataFrame
duplicate.to_csv('20.4_booklist.csv', index=False)












